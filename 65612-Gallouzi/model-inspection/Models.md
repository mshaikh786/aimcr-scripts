# Models Mentioned in the Proposal

## Explicitly Named Foundation / LLM Models

Appears in model scaling comparison and performance discussion

- [LLaMA 3.2 – 3B](#llama-32--3b)

---

## LLaMA 3.2 – 3B
**Family:** LLaMA 3.2  
**Source:** Meta AI / Hugging Face  
**Documentation / Model Card Links:**  
- https://huggingface.co/meta-llama/Llama-3.2-3B

---

### 1. Source / Provenance

#### 1.1 Where is it coming from?

LLaMA 3.2 – 3B is part of the Meta AI **LLaMA 3.2** family of pretrained and instruction-tuned language models hosted 
publicly on Hugging Face. It is developed and published by **Meta AI**, the research organization within Meta (formerly 
Facebook) responsible for large language model research and releases.

#### 1.2 Who owns it?
- **Owner:** Meta AI  
- **Organization:** Meta Platforms, Inc.

---

### 2. Model Characteristics

#### 2.1 Number of Trainable Parameters
- **Approximate parameter count:** ~3 billion  
  - The model is described as a “3B” variant, meaning ~3.2 billion parameters in its instruction-tuned form.

---

#### 2.2 Training Token Count
- **Training data scale:**  
  - LLaMA 3.2 models were pretrained on up to **9 trillion tokens** of text from publicly available sources. 
  - Broader LLaMA 3.2 family (including larger variants) is trained on significantly larger corpora, but specific 
  documentation for the 3B variant indicates this upper bound.

---

#### 2.3 MFU / Training Compute Utilization
- **Model FLOPs or MFU:**  
  - Public documentation does not provide official Meta MFU (Megaflop Utilization) figures for the 3B variant. In absence of an official metric, such utilization must be inferred from training dataset size and parameter count.  
  - **No published official MFU benchmarks** specific to this model were found in Meta’s public model cards or docs.

---